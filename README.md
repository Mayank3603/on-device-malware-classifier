# On Device Testing

We conducted testing of the trained models on actual Android phones. For this experiment, we utilized a Motorola G40 running Android v12. TensorFlow Lite conversion was used to test the models on the device, as Android does not support the TensorFlow library. TensorFlow Lite models are optimized for mobile and edge devices, offering faster inference and reduced memory footprint compared to Keras models, making them more suitable for deployment on resource-constrained platforms.

We deployed an APK to test one of the models on the Android phone itself. The results are presented below:

## Experiment Details

- **Training-Testing Split**: 50:50
- **Testing Split**: 90:10 (BW:MW)

## Performance Metrics

| Models          | Feature Count | Testing APKs | Accuracy | Memory Usage (%) | Inference Time (s) |
|-----------------|---------------|--------------|----------|------------------|--------------------|
| Original DNN    | 934           | 10k          | 0.95     | 2.8              | 6.734              |
| DNN (LIME Top Half Union)       | 672           | 10k          | 0.94     | 2.2              | 5.011              |
| DNN (LIME Avg Intersection)     | 69            | 10k          | 0.94     | 1.8              | 1.757              |

**Table 5.3:** Performance of Lightweight DNN model on the device itself.

Feature reduction led to an overall decrease of 68.15% and 35.71% in inference time and memory usage respectively. For a more nuanced analysis, we also tested with a single APK and repeated it multiple times. It took 0.00067s to draw inference on the APK using the original DNN model, and decreased to 0.00021s with the lightweight DNN model.
